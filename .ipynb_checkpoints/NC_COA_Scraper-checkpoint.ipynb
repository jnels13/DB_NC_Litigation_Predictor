{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NC Court of Appeals Scraper\n",
    "\n",
    "This notebook scrapes the NC Court of Appeals PDF opinions and convert them into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pickle\n",
    "from pdfminer3.layout import LAParams, LTTextBox\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from pdfminer3.pdfinterp import PDFResourceManager\n",
    "from pdfminer3.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer3.converter import PDFPageAggregator\n",
    "from pdfminer3.converter import TextConverter\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get List of All Opinions (Stored as PDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a get request to retrieve the court of appeals' page\n",
    "html_page = requests.get('https://appellate.nccourts.org/opinion-filings/?c=coa&year=2020') \n",
    "# Pass the page contents for parsing\n",
    "soup = BeautifulSoup(html_page.content, 'html.parser') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.findAll('span', class_=\"title\")\n",
    "titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[0].attrs['onclick']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[0].attrs['onclick'].strip('viewOpinion(\\\"').strip('\\\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_links = [titles.attrs['onclick'].strip('viewOpinion(\\\"').strip('\\\")') for titles in soup.findAll('span', class_='title')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(page_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape all PDF links (web site stores links from 1998 to the present)\n",
    "all_links = []\n",
    "for i in range(1998,2021,1):\n",
    "    cur_year = 'https://appellate.nccourts.org/opinion-filings/?c=coa&year={}'.format(i)\n",
    "    html_page = requests.get(cur_year)\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser') \n",
    "    page_links = [titles.attrs['onclick'].strip('viewOpinion(\\\"').strip('\\\")') for titles in soup.findAll('span', class_='title')]\n",
    "    all_links += page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of opinions to be scraped\n",
    "length = len(all_links)\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete any possible duplicates\n",
    "set(all_links) \n",
    "len(all_links)\n",
    "list(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of PDFs to data file\n",
    "with open('pdf_addresses.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(all_links, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Function that Downloads All of the PDF Opinions\n",
    "\n",
    "The COA's web page contains entries for 1998-2020.  Create a function that appends all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scrape all opinions\n",
    "# this function run alone timed out after downloading 4,484 of the 28,861 \n",
    "\n",
    "for i in range(length):\n",
    "    target = all_links[i]\n",
    "    name = target[-5:].replace('=','0') # name for the saved PDF, adding leading 0 to four-digit PDF names\n",
    "\n",
    "    r = requests.get(target) # create HTTP response object \n",
    "\n",
    "    with open(\"Scraped_PDFs/\"+name+\".pdf\",'wb') as f: \n",
    "        f.write(r.content) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Received error at PDF named 21317; received 4,484 items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Proxies to Scrape PDF List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proxies import get_proxies, check_proxies #Attempt these before going with a paid proxy\n",
    "working_proxies = check_proxies()\n",
    "# import pickle file of all current PDFs. \n",
    "infile = open('pdf_addresses.data','rb')\n",
    "pdf_addresses = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert PDFs into Text Strings and Build Into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from Stack Overflow to use PDFminer to import PDF to text reworked into a function\n",
    "resource_manager = PDFResourceManager()\n",
    "fake_file_handle = io.StringIO()\n",
    "converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "\n",
    "def pdf2txt(path):\n",
    "    \"\"\"\n",
    "    A function to convert a PDF to a string, \n",
    "    takes the file path to the PDF as the \n",
    "    argument and returns the PDF as a string.\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as fh:\n",
    "\n",
    "        for page in PDFPage.get_pages(fh,\n",
    "                                      caching=True,\n",
    "                                      check_extractable=True):\n",
    "            page_interpreter.process_page(page)\n",
    "\n",
    "        text = fake_file_handle.getvalue()\n",
    "\n",
    "    # close open handles\n",
    "    converter.close()\n",
    "    fake_file_handle.close()\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'SampleData/SamplePDF.pdf'\n",
    "text = pdf2txt(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text  #WORKS!!\n",
    "type(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
